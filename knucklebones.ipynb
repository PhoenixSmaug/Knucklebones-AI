{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Knucklebones AI\n",
    "\n",
    "Knucklebones is a highly random dice game developed by the studio [Massive Monster](https://massivemonster.co) available to play online [here](https://knucklebones.io). There you can also find a summary of the rules of the game; a more detailed explanation also given on the [Fandom Wiki](https://cult-of-the-lamb.fandom.com/wiki/Knucklebones). The goal of this project is to train an AI by self play using the Reinforcement Learning library [Stable-Baselines3](https://stable-baselines3.readthedocs.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by modelling the game according to OpenAI's Gym Standard for Reinforcement Learning. The RL model will provide a ranking of its preferences for the three options (encoded as one of the 6 permutations) and the game will chooses the valid actions with the highest preference. This prevents the AI from making invalid actions, so placing a dice in an already full column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnucklebonesEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(KnucklebonesEnv, self).__init__()\n",
    "        self.board = np.zeros((2, 3, 3), dtype=int)\n",
    "        self.current_player = 0\n",
    "        self.current_dice = None\n",
    "\n",
    "        # action indicates preferences between three columns encoded as permutation\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        self.observation_space = spaces.Box(low=0, high=6, shape=(19,), dtype=np.int32)\n",
    "\n",
    "        # Define the permutations\n",
    "        self.permutations = [\n",
    "            [0, 1, 2],\n",
    "            [0, 2, 1],\n",
    "            [1, 0, 2],\n",
    "            [1, 2, 0],\n",
    "            [2, 0, 1],\n",
    "            [2, 1, 0]\n",
    "        ]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.board = np.zeros((2, 3, 3), dtype=int)\n",
    "        self.current_player = 0\n",
    "        self.current_dice = self.roll_dice()\n",
    "        return self.get_observation(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Convert the action to a permutation\n",
    "        column_order = self.permutations[action]\n",
    "\n",
    "        for column in column_order:\n",
    "            if self.is_valid_action(column):\n",
    "                self.place_dice(self.current_player, column, self.current_dice)\n",
    "                self.remove_opponent_dice(1 - self.current_player, column, self.current_dice)\n",
    "                break\n",
    "\n",
    "        terminated = self.is_game_over()\n",
    "        reward = self.calculate_score(self.current_player)\n",
    "        \n",
    "        self.current_player = 1 - self.current_player\n",
    "        self.current_dice = self.roll_dice()\n",
    "\n",
    "        return self.get_observation(), float(reward), bool(terminated), False, {}\n",
    "\n",
    "    def roll_dice(self):\n",
    "        return self.np_random.integers(1, 7)\n",
    "\n",
    "    def is_valid_action(self, action):\n",
    "        return 0 <= action < 3 and np.any(self.board[self.current_player, action] == 0)\n",
    "\n",
    "    def place_dice(self, player, column, value):\n",
    "        empty_spots = np.where(self.board[player, column] == 0)[0]\n",
    "        if empty_spots.size > 0:\n",
    "            self.board[player, column, empty_spots[0]] = value\n",
    "\n",
    "    def remove_opponent_dice(self, opponent, column, value):\n",
    "        self.board[opponent, column] = np.where(self.board[opponent, column] == value, 0, self.board[opponent, column])\n",
    "        self.board[opponent, column] = np.sort(self.board[opponent, column])[::-1]\n",
    "\n",
    "    def is_game_over(self):\n",
    "        return np.all(self.board[0] != 0) or np.all(self.board[1] != 0)\n",
    "\n",
    "    def calculate_score(self, player):\n",
    "        score = 0\n",
    "        for column in self.board[player]:\n",
    "            unique, counts = np.unique(column[column != 0], return_counts=True)\n",
    "            for value, count in zip(unique, counts):\n",
    "                score += value * count\n",
    "        return score\n",
    "\n",
    "    def get_observation(self):\n",
    "        return np.concatenate([self.board.flatten(), [self.current_dice]]).astype(np.int32)\n",
    "\n",
    "    def render(self):\n",
    "        for player in range(2):\n",
    "            print(f\"Player {player + 1}:\")\n",
    "            for row in range(3):\n",
    "                print(\" \".join(f\"{self.board[player, col, row]:2d}\" for col in range(3)))\n",
    "            print()\n",
    "        print(f\"Current player: {self.current_player + 1}, Current dice: {self.current_dice}\")\n",
    "\n",
    "    def get_current_player(self):\n",
    "        return self.current_player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will be trained using self-play, so by adversarial playing against itself across thousands of games. Since Stable-Baselines3 does not have native support for environments with multiple agents, we wrap the game environment in a multienvironment containing a separate opponent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnucklebonesMultiEnv(KnucklebonesEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.player_1_obs = None\n",
    "        self.player_2_obs = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        obs, info = super().reset(seed=seed, options=options)\n",
    "        self.player_1_obs = obs\n",
    "        self.player_2_obs = self._flip_observation(obs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = super().step(action)\n",
    "        self.player_1_obs = obs if self.current_player == 0 else self._flip_observation(obs)\n",
    "        self.player_2_obs = obs if self.current_player == 1 else self._flip_observation(obs)\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def _flip_observation(self, obs):\n",
    "        flipped_obs = obs.copy()\n",
    "        flipped_obs[:9], flipped_obs[9:18] = obs[9:18], obs[:9]\n",
    "        return flipped_obs\n",
    "\n",
    "    def get_player_obs(self, player):\n",
    "        return self.player_1_obs if player == 0 else self.player_2_obs\n",
    "\n",
    "class SelfPlayEnv(gym.Env):\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.opponent = None\n",
    "        self.action_space = env.action_space\n",
    "        self.observation_space = env.observation_space\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        return self.env.reset(seed=seed, options=options)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        if not done:\n",
    "            if self.opponent:\n",
    "                opponent_obs = self.env.get_player_obs(self.env.get_current_player())\n",
    "                opponent_action, _ = self.opponent.predict(opponent_obs, deterministic=True)\n",
    "                obs, reward_op, done, truncated, info = self.env.step(opponent_action)\n",
    "                # reward as difference to opponent score\n",
    "                reward -= reward_op\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def set_opponent(self, opponent):\n",
    "        self.opponent = opponent\n",
    "\n",
    "    def render(self):\n",
    "        return self.env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reward function we will use the difference between the score of the model player and the score of the opponent in the next round. The aim is to encourage the model to maximize its own score but also prevent the opponent from achieving a high score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Mia Müßig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
