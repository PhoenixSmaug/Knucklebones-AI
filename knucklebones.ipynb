{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Knucklebones AI\n",
    "\n",
    "Knucklebones is a highly random dice game developed by the studio [Massive Monster](https://massivemonster.co) available to play online [here](https://knucklebones.io). There you can also find a summary of the rules of the game; a more detailed explanation also given on the [Fandom Wiki](https://cult-of-the-lamb.fandom.com/wiki/Knucklebones). The goal of this project is to train an AI for Knucklebones using various methods and compare the resulting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using POMDPs\n",
    "using Random\n",
    "using StaticArrays\n",
    "using MCTS\n",
    "using LinearAlgebra\n",
    "using DataStructures\n",
    "using Flux\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Definition\n",
    "\n",
    "We start by modelling the game as a Markov Decision Process using the [POMDPs.jl interface](https://github.com/JuliaPOMDP/POMDPs.jl). Our models will provide a ranking of their preferences for the three options (encoded as one of the 6 permutations) and the game will chooses the valid actions with the highest preference. This prevents the AI from making invalid actions, so placing a dice in an already full column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct KnucklebonesState\n",
    "    board::SMatrix{2,3,SVector{3,Int},6}\n",
    "    current_player::Int\n",
    "    current_dice::Int\n",
    "end\n",
    "\n",
    "struct KnucklebonesMDP <: MDP{KnucklebonesState, Int}\n",
    "    discount_factor::Float64\n",
    "end\n",
    "\n",
    "POMDPs.discount(mdp::KnucklebonesMDP) = mdp.discount_factor\n",
    "\n",
    "# Define permutations\n",
    "const PERMUTATIONS = [\n",
    "    [1, 2, 3],\n",
    "    [1, 3, 2],\n",
    "    [2, 1, 3],\n",
    "    [2, 3, 1],\n",
    "    [3, 1, 2],\n",
    "    [3, 2, 1]\n",
    "]\n",
    "\n",
    "function POMDPs.gen(mdp::KnucklebonesMDP, s::KnucklebonesState, a::Int, rng::AbstractRNG)\n",
    "    new_board = copy(s.board)\n",
    "    player = s.current_player\n",
    "    opponent = 3 - player\n",
    "\n",
    "    # Find the first valid column based on the action's permutation\n",
    "    column = first(filter(c -> any(new_board[player, c] .== 0), PERMUTATIONS[a]))\n",
    "\n",
    "    # Place dice\n",
    "    for row in 1:3\n",
    "        if new_board[player, column][row] == 0\n",
    "            new_board = setindex(new_board, setindex(new_board[player, column], s.current_dice, row), player, column)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Remove opponent's dice\n",
    "    opponent_column = new_board[opponent, column]\n",
    "    new_opponent_column = sort(filter(x -> x != s.current_dice, opponent_column), rev=true)\n",
    "    new_opponent_column = vcat(new_opponent_column, zeros(Int, 3 - length(new_opponent_column)))\n",
    "    new_board = setindex(new_board, SVector{3,Int}(new_opponent_column), opponent, column)\n",
    "\n",
    "    # Calculate reward\n",
    "    player_score = calculate_score(new_board[player, :])\n",
    "    opponent_score = calculate_score(new_board[opponent, :])\n",
    "    reward = player_score - opponent_score\n",
    "\n",
    "    # New state\n",
    "    new_state = KnucklebonesState(new_board, opponent, rand(rng, 1:6))\n",
    "\n",
    "    return (sp=new_state, r=reward)\n",
    "end\n",
    "\n",
    "function calculate_score(player_board)\n",
    "    score = 0\n",
    "    for column in player_board\n",
    "        unique_vals = unique(filter(!iszero, column))\n",
    "        for val in unique_vals\n",
    "            c = count(==(val), column)\n",
    "            score += val * c * c\n",
    "        end\n",
    "    end\n",
    "    return score\n",
    "end\n",
    "\n",
    "POMDPs.actions(mdp::KnucklebonesMDP, s::KnucklebonesState) = 1:6\n",
    "\n",
    "function POMDPs.isterminal(mdp::KnucklebonesMDP, s::KnucklebonesState)\n",
    "    return playerFinished(mdp, s, 1) || playerFinished(mdp, s, 2)\n",
    "end\n",
    "\n",
    "function playerFinished(mdp::KnucklebonesMDP, s::KnucklebonesState, p::Int)\n",
    "    for row in 1:3\n",
    "        for col in 1 : 3\n",
    "            if iszero(s.board[p, col][row])\n",
    "                return false  # player p can still play\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return true\n",
    "end\n",
    "\n",
    "function POMDPs.initialstate(mdp::KnucklebonesMDP)\n",
    "    return KnucklebonesState(\n",
    "        @SMatrix[SVector(0,0,0) SVector(0,0,0) SVector(0,0,0);\n",
    "                 SVector(0,0,0) SVector(0,0,0) SVector(0,0,0)],\n",
    "        1,\n",
    "        rand(1:6)\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reward function we will use the difference between the score of the model player and the score of the opponent in the next round. The aim is to encourage the model to maximize its own score but also prevent the opponent from achieving a high score. Now we need a function to evaluate two models playing against each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate_policies (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function play_game(mdp::KnucklebonesMDP, policy1, policy2)\n",
    "    s = initialstate(mdp)\n",
    "\n",
    "    while !isterminal(mdp, s)\n",
    "        a = s.current_player == 1 ? action(policy1, s) : action(policy2, s)\n",
    "        s, _ = gen(mdp, s, a, Random.GLOBAL_RNG)\n",
    "    end\n",
    "\n",
    "    player1_score = calculate_score(s.board[1, :])\n",
    "    player2_score = calculate_score(s.board[2, :])\n",
    "    return player1_score - player2_score\n",
    "end\n",
    "\n",
    "function evaluate_policies(mdp::KnucklebonesMDP, policy1, policy2, n_games::Int)\n",
    "    scores = [play_game(mdp, policy1, policy2) for _ in 1:n_games]\n",
    "    policy1_wins = count(s -> s > 0, scores)\n",
    "    policy2_wins = count(s -> s < 0, scores)\n",
    "    draws = count(iszero, scores)\n",
    "\n",
    "    println(\"Policy 1 wins: $policy1_wins\")\n",
    "    println(\"Policy 2 wins: $policy2_wins\")\n",
    "    println(\"Draws: $draws\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that our game implementation is working correctly, we can have a look at an example game between models making uniformly random actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player 1:\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 0\n",
      "\n",
      "Player 2:\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 0\n",
      "\n",
      "Current player: 1, Current dice: 5\n",
      "Player 1 chooses action 2\n",
      "Reward: 5\n",
      "------------------------\n",
      "Player 1:\n",
      "5 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 5\n",
      "\n",
      "Player 2:\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 0\n",
      "\n",
      "Current player: 2, Current dice: 2\n",
      "Player 2 chooses action 4\n",
      "Reward: -3\n",
      "------------------------\n",
      "Player 1:\n",
      "5 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 5\n",
      "\n",
      "Player 2:\n",
      "0 2 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 2\n",
      "\n",
      "Current player: 1, Current dice: 1\n",
      "Player 1 chooses action 5\n",
      "Reward: 4\n",
      "------------------------\n",
      "Player 1:\n",
      "5 0 1\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 6\n",
      "\n",
      "Player 2:\n",
      "0 2 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 2\n",
      "\n",
      "Current player: 2, Current dice: 4\n",
      "Player 2 chooses action 6\n",
      "Reward: 0\n",
      "------------------------\n",
      "Player 1:\n",
      "5 0 1\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 6\n",
      "\n",
      "Player 2:\n",
      "0 2 4\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 6\n",
      "\n",
      "Current player: 1, Current dice: 5\n",
      "Player 1 chooses action 5\n",
      "Reward: 5\n",
      "------------------------\n",
      "Player 1:\n",
      "5 0 1\n",
      "0 0 5\n",
      "0 0 0\n",
      "Score: 11\n",
      "\n",
      "Player 2:\n",
      "0 2 4\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 6\n",
      "\n",
      "Current player: 2, Current dice: 1\n",
      "Player 2 chooses action 1\n",
      "Reward: -4\n",
      "------------------------\n",
      "Player 1:\n",
      "5 0 1\n",
      "0 0 5\n",
      "0 0 0\n",
      "Score: 11\n",
      "\n",
      "Player 2:\n",
      "1 2 4\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 7\n",
      "\n",
      "Current player: 1, Current dice: 5\n",
      "Player 1 chooses action 5\n",
      "Reward: 19\n",
      "------------------------\n",
      "Player 1:\n",
      "5 0 1\n",
      "0 0 5\n",
      "0 0 5\n",
      "Score: 26\n",
      "\n",
      "Player 2:\n",
      "1 2 4\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 7\n",
      "\n",
      "Current player: 2, Current dice: 6\n",
      "Player 2 chooses action 4\n",
      "Reward: -13\n",
      "------------------------\n",
      "Player 1:\n",
      "5 0 1\n",
      "0 0 5\n",
      "0 0 5\n",
      "Score: 26\n",
      "\n",
      "Player 2:\n",
      "1 2 4\n",
      "0 6 0\n",
      "0 0 0\n",
      "Score: 13\n",
      "\n",
      "Current player: 1, Current dice: 5\n",
      "Player 1 chooses action 4\n",
      "Reward: 18\n",
      "------------------------\n",
      "Player 1:\n",
      "5 5 1\n",
      "0 0 5\n",
      "0 0 5\n",
      "Score: 31\n",
      "\n",
      "Player 2:\n",
      "1 6 4\n",
      "0 2 0\n",
      "0 0 0\n",
      "Score: 13\n",
      "\n",
      "Current player: 2, Current dice: 2\n",
      "Player 2 chooses action 3\n",
      "Reward: -12\n",
      "------------------------\n",
      "Player 1:\n",
      "5 5 1\n",
      "0 0 5\n",
      "0 0 5\n",
      "Score: 31\n",
      "\n",
      "Player 2:\n",
      "1 6 4\n",
      "0 2 0\n",
      "0 2 0\n",
      "Score: 19\n",
      "\n",
      "Current player: 1, Current dice: 4\n",
      "Player 1 chooses action 5\n",
      "Reward: 16\n",
      "------------------------\n",
      "Player 1:\n",
      "5 5 1\n",
      "4 0 5\n",
      "0 0 5\n",
      "Score: 35\n",
      "\n",
      "Player 2:\n",
      "1 6 4\n",
      "0 2 0\n",
      "0 2 0\n",
      "Score: 19\n",
      "\n",
      "Current player: 2, Current dice: 4\n",
      "Player 2 chooses action 5\n",
      "Reward: -4\n",
      "------------------------\n",
      "Player 1:\n",
      "5 5 5\n",
      "4 0 5\n",
      "0 0 1\n",
      "Score: 35\n",
      "\n",
      "Player 2:\n",
      "1 6 4\n",
      "0 2 4\n",
      "0 2 0\n",
      "Score: 31\n",
      "\n",
      "Current player: 1, Current dice: 2\n",
      "Player 1 chooses action 6\n",
      "Reward: 14\n",
      "------------------------\n",
      "Player 1:\n",
      "5 5 5\n",
      "4 2 5\n",
      "0 0 1\n",
      "Score: 37\n",
      "\n",
      "Player 2:\n",
      "1 6 4\n",
      "0 0 4\n",
      "0 0 0\n",
      "Score: 23\n",
      "\n",
      "Current player: 2, Current dice: 6\n",
      "Player 2 chooses action 6\n",
      "Reward: -8\n",
      "------------------------\n",
      "Player 1:\n",
      "5 5 5\n",
      "4 2 5\n",
      "0 0 1\n",
      "Score: 37\n",
      "\n",
      "Player 2:\n",
      "1 6 4\n",
      "0 0 4\n",
      "0 0 6\n",
      "Score: 29\n",
      "\n",
      "Current player: 1, Current dice: 1\n",
      "Player 1 chooses action 3\n",
      "Reward: 9\n",
      "------------------------\n",
      "Player 1:\n",
      "5 5 5\n",
      "4 2 5\n",
      "0 1 1\n",
      "Score: 38\n",
      "\n",
      "Player 2:\n",
      "1 6 4\n",
      "0 0 4\n",
      "0 0 6\n",
      "Score: 29\n",
      "\n",
      "Current player: 2, Current dice: 5\n",
      "Player 2 chooses action 5\n",
      "Reward: 1\n",
      "------------------------\n",
      "Player 1:\n",
      "4 5 5\n",
      "0 2 5\n",
      "0 1 1\n",
      "Score: 33\n",
      "\n",
      "Player 2:\n",
      "1 6 4\n",
      "5 0 4\n",
      "0 0 6\n",
      "Score: 34\n",
      "\n",
      "Current player: 1, Current dice: 1\n",
      "Player 1 chooses action 5\n",
      "Reward: 1\n",
      "------------------------\n",
      "Player 1:\n",
      "4 5 5\n",
      "1 2 5\n",
      "0 1 1\n",
      "Score: 34\n",
      "\n",
      "Player 2:\n",
      "5 6 4\n",
      "0 0 4\n",
      "0 0 6\n",
      "Score: 33\n",
      "\n",
      "Current player: 2, Current dice: 3\n",
      "Player 2 chooses action 5\n",
      "Reward: 2\n",
      "------------------------\n",
      "Player 1:\n",
      "4 5 5\n",
      "1 2 5\n",
      "0 1 1\n",
      "Score: 34\n",
      "\n",
      "Player 2:\n",
      "5 6 4\n",
      "3 0 4\n",
      "0 0 6\n",
      "Score: 36\n",
      "\n",
      "Current player: 1, Current dice: 5\n",
      "Player 1 chooses action 4\n",
      "Reward: 8\n",
      "------------------------\n",
      "Player 1:\n",
      "4 5 5\n",
      "1 2 5\n",
      "5 1 1\n",
      "Score: 39\n",
      "\n",
      "Player 2:\n",
      "3 6 4\n",
      "0 0 4\n",
      "0 0 6\n",
      "Score: 31\n",
      "\n",
      "Current player: 2, Current dice: 5\n",
      "Game Over\n",
      "Final Scores - Player 1: 39, Player 2: 31\n",
      "Player 1 wins!\n"
     ]
    }
   ],
   "source": [
    "function print_board(s::KnucklebonesState)\n",
    "    for player in 1:2\n",
    "        println(\"Player $player:\")\n",
    "        for row in 1:3\n",
    "            println(join([s.board[player, col][row] for col in 1:3], \" \"))\n",
    "        end\n",
    "        println(\"Score: $(calculate_score(s.board[player, :]))\")\n",
    "        println()\n",
    "    end\n",
    "    println(\"Current player: $(s.current_player), Current dice: $(s.current_dice)\")\n",
    "end\n",
    "\n",
    "function play_example_game(mdp::KnucklebonesMDP, policy1, policy2)\n",
    "    s = initialstate(mdp)\n",
    "    \n",
    "    while !isterminal(mdp, s)\n",
    "        print_board(s)\n",
    "        a = s.current_player == 1 ? action(policy1, s) : action(policy2, s)\n",
    "        println(\"Player $(s.current_player) chooses action $a\")\n",
    "        s, r = gen(mdp, s, a, Random.GLOBAL_RNG)\n",
    "        println(\"Reward: $r\")\n",
    "        println(\"------------------------\")\n",
    "    end\n",
    "    \n",
    "    print_board(s)\n",
    "    println(\"Game Over\")\n",
    "    \n",
    "    player1_score = calculate_score(s.board[1, :])\n",
    "    player2_score = calculate_score(s.board[2, :])\n",
    "    println(\"Final Scores - Player 1: $player1_score, Player 2: $player2_score\")\n",
    "    if player1_score > player2_score\n",
    "        println(\"Player 1 wins!\")\n",
    "    elseif player2_score > player1_score\n",
    "        println(\"Player 2 wins!\")\n",
    "    else\n",
    "        println(\"It's a draw!\")\n",
    "    end\n",
    "end\n",
    "\n",
    "struct RandomPolicy <: Policy end\n",
    "POMDPs.action(::RandomPolicy, s::KnucklebonesState) = rand(1:6)\n",
    "\n",
    "mdp = KnucklebonesMDP(0.95)\n",
    "random_policy = RandomPolicy()\n",
    "\n",
    "play_example_game(mdp, random_policy, random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo tree search\n",
    "\n",
    "For our first model, we will employ the heuristic search algorithm [MCTS](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search), which is very well suited for highly random games like Knucklebones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MCTSPlanner{KnucklebonesMDP, KnucklebonesState, Int64, MCTS.SolvedRolloutEstimator{POMDPTools.Policies.RandomPolicy{Random._GLOBAL_RNG, KnucklebonesMDP, POMDPTools.BeliefUpdaters.NothingUpdater}, Random._GLOBAL_RNG}, Random._GLOBAL_RNG}(MCTSSolver(1000, Inf, 20, 5.0, Random._GLOBAL_RNG(), RolloutEstimator(POMDPTools.Policies.RandomSolver(Random._GLOBAL_RNG()), 50, 0.0), 0.0, 0, false, false, MCTS.var\"#5#7\"()), KnucklebonesMDP(0.95), MCTS.MCTSTree{KnucklebonesState, Int64}(Dict{KnucklebonesState, Int64}(), Vector{Int64}[], Int64[], KnucklebonesState[], Int64[], Float64[], Int64[], Dict{Pair{Int64, Int64}, Int64}()), MCTS.SolvedRolloutEstimator{POMDPTools.Policies.RandomPolicy{Random._GLOBAL_RNG, KnucklebonesMDP, POMDPTools.BeliefUpdaters.NothingUpdater}, Random._GLOBAL_RNG}(POMDPTools.Policies.RandomPolicy{Random._GLOBAL_RNG, KnucklebonesMDP, POMDPTools.BeliefUpdaters.NothingUpdater}(Random._GLOBAL_RNG(), KnucklebonesMDP(0.95), POMDPTools.BeliefUpdaters.NothingUpdater()), Random._GLOBAL_RNG(), 50, 0.0), Random._GLOBAL_RNG())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function train_mcts(mdp::KnucklebonesMDP, n_iterations::Int, exploration_constant::Float64)\n",
    "    solver = MCTSSolver(n_iterations=n_iterations, depth=20, exploration_constant=exploration_constant)\n",
    "    return solve(solver, mdp)\n",
    "end\n",
    "\n",
    "mcts_policy = train_mcts(mdp, 1000, 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating MCTS against Random:\n",
      "Policy 1 wins: 163\n",
      "Policy 2 wins: 35\n",
      "Draws: 2\n",
      "\n",
      "Playing an example game (MCTS vs Random):\n",
      "Player 1:\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 0\n",
      "\n",
      "Player 2:\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 0\n",
      "\n",
      "Current player: 1, Current dice: 1\n",
      "Player 1 chooses action 5\n",
      "Reward: 1\n",
      "------------------------\n",
      "Player 1:\n",
      "0 0 1\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 1\n",
      "\n",
      "Player 2:\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 0\n",
      "\n",
      "Current player: 2, Current dice: 4\n",
      "Player 2 chooses action 5\n",
      "Reward: 3\n",
      "------------------------\n",
      "Player 1:\n",
      "0 0 1\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 1\n",
      "\n",
      "Player 2:\n",
      "0 0 4\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 4\n",
      "\n",
      "Current player: 1, Current dice: 4\n",
      "Player 1 chooses action 6\n",
      "Reward: 5\n",
      "------------------------\n",
      "Player 1:\n",
      "0 0 1\n",
      "0 0 4\n",
      "0 0 0\n",
      "Score: 5\n",
      "\n",
      "Player 2:\n",
      "0 0 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 0\n",
      "\n",
      "Current player: 2, Current dice: 1\n",
      "Player 2 chooses action 3\n",
      "Reward: -4\n",
      "------------------------\n",
      "Player 1:\n",
      "0 0 1\n",
      "0 0 4\n",
      "0 0 0\n",
      "Score: 5\n",
      "\n",
      "Player 2:\n",
      "0 1 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 1\n",
      "\n",
      "Current player: 1, Current dice: 5\n",
      "Player 1 chooses action 1\n",
      "Reward: 9\n",
      "------------------------\n",
      "Player 1:\n",
      "5 0 1\n",
      "0 0 4\n",
      "0 0 0\n",
      "Score: 10\n",
      "\n",
      "Player 2:\n",
      "0 1 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 1\n",
      "\n",
      "Current player: 2, Current dice: 5\n",
      "Player 2 chooses action 2\n",
      "Reward: 1\n",
      "------------------------\n",
      "Player 1:\n",
      "0 0 1\n",
      "0 0 4\n",
      "0 0 0\n",
      "Score: 5\n",
      "\n",
      "Player 2:\n",
      "5 1 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 6\n",
      "\n",
      "Current player: 1, Current dice: 2\n",
      "Player 1 chooses action 4\n",
      "Reward: 1\n",
      "------------------------\n",
      "Player 1:\n",
      "0 2 1\n",
      "0 0 4\n",
      "0 0 0\n",
      "Score: 7\n",
      "\n",
      "Player 2:\n",
      "5 1 0\n",
      "0 0 0\n",
      "0 0 0\n",
      "Score: 6\n",
      "\n",
      "Current player: 2, Current dice: 1\n",
      "Player 2 chooses action 3\n",
      "Reward: 2\n",
      "------------------------\n",
      "Player 1:\n",
      "0 2 1\n",
      "0 0 4\n",
      "0 0 0\n",
      "Score: 7\n",
      "\n",
      "Player 2:\n",
      "5 1 0\n",
      "0 1 0\n",
      "0 0 0\n",
      "Score: 9\n",
      "\n",
      "Current player: 1, Current dice: 6\n",
      "Player 1 chooses action 4\n",
      "Reward: 4\n",
      "------------------------\n",
      "Player 1:\n",
      "0 2 1\n",
      "0 6 4\n",
      "0 0 0\n",
      "Score: 13\n",
      "\n",
      "Player 2:\n",
      "5 1 0\n",
      "0 1 0\n",
      "0 0 0\n",
      "Score: 9\n",
      "\n",
      "Current player: 2, Current dice: 5\n",
      "Player 2 chooses action 6\n",
      "Reward: 1\n",
      "------------------------\n",
      "Player 1:\n",
      "0 2 4\n",
      "0 6 1\n",
      "0 0 0\n",
      "Score: 13\n",
      "\n",
      "Player 2:\n",
      "5 1 5\n",
      "0 1 0\n",
      "0 0 0\n",
      "Score: 14\n",
      "\n",
      "Current player: 1, Current dice: 4\n",
      "Player 1 chooses action 5\n",
      "Reward: 11\n",
      "------------------------\n",
      "Player 1:\n",
      "0 2 4\n",
      "0 6 1\n",
      "0 0 4\n",
      "Score: 25\n",
      "\n",
      "Player 2:\n",
      "5 1 5\n",
      "0 1 0\n",
      "0 0 0\n",
      "Score: 14\n",
      "\n",
      "Current player: 2, Current dice: 4\n",
      "Player 2 chooses action 3\n",
      "Reward: -7\n",
      "------------------------\n",
      "Player 1:\n",
      "0 6 4\n",
      "0 2 1\n",
      "0 0 4\n",
      "Score: 25\n",
      "\n",
      "Player 2:\n",
      "5 1 5\n",
      "0 1 0\n",
      "0 4 0\n",
      "Score: 18\n",
      "\n",
      "Current player: 1, Current dice: 3\n",
      "Player 1 chooses action 5\n",
      "Reward: 10\n",
      "------------------------\n",
      "Player 1:\n",
      "3 6 4\n",
      "0 2 1\n",
      "0 0 4\n",
      "Score: 28\n",
      "\n",
      "Player 2:\n",
      "5 1 5\n",
      "0 1 0\n",
      "0 4 0\n",
      "Score: 18\n",
      "\n",
      "Current player: 2, Current dice: 6\n",
      "Player 2 chooses action 5\n",
      "Reward: -4\n",
      "------------------------\n",
      "Player 1:\n",
      "3 6 4\n",
      "0 2 4\n",
      "0 0 1\n",
      "Score: 28\n",
      "\n",
      "Player 2:\n",
      "5 1 5\n",
      "0 1 6\n",
      "0 4 0\n",
      "Score: 24\n",
      "\n",
      "Current player: 1, Current dice: 3\n",
      "Player 1 chooses action 1\n",
      "Reward: 13\n",
      "------------------------\n",
      "Player 1:\n",
      "3 6 4\n",
      "3 2 4\n",
      "0 0 1\n",
      "Score: 37\n",
      "\n",
      "Player 2:\n",
      "5 1 5\n",
      "0 1 6\n",
      "0 4 0\n",
      "Score: 24\n",
      "\n",
      "Current player: 2, Current dice: 6\n",
      "Player 2 chooses action 6\n",
      "Reward: 5\n",
      "------------------------\n",
      "Player 1:\n",
      "3 6 4\n",
      "3 2 4\n",
      "0 0 1\n",
      "Score: 37\n",
      "\n",
      "Player 2:\n",
      "5 1 5\n",
      "0 1 6\n",
      "0 4 6\n",
      "Score: 42\n",
      "\n",
      "Current player: 1, Current dice: 2\n",
      "Player 1 chooses action 6\n",
      "Reward: 1\n",
      "------------------------\n",
      "Player 1:\n",
      "3 6 4\n",
      "3 2 4\n",
      "0 2 1\n",
      "Score: 43\n",
      "\n",
      "Player 2:\n",
      "5 4 5\n",
      "0 1 6\n",
      "0 1 6\n",
      "Score: 42\n",
      "\n",
      "Current player: 2, Current dice: 6\n",
      "Player 2 chooses action 5\n",
      "Reward: 5\n",
      "------------------------\n",
      "Player 1:\n",
      "3 6 4\n",
      "3 2 4\n",
      "0 2 1\n",
      "Score: 43\n",
      "\n",
      "Player 2:\n",
      "5 4 5\n",
      "6 1 6\n",
      "0 1 6\n",
      "Score: 48\n",
      "\n",
      "Current player: 1, Current dice: 4\n",
      "Player 1 chooses action 1\n",
      "Reward: -1\n",
      "------------------------\n",
      "Player 1:\n",
      "3 6 4\n",
      "3 2 4\n",
      "4 2 1\n",
      "Score: 47\n",
      "\n",
      "Player 2:\n",
      "6 4 5\n",
      "5 1 6\n",
      "0 1 6\n",
      "Score: 48\n",
      "\n",
      "Current player: 2, Current dice: 4\n",
      "Game Over\n",
      "Final Scores - Player 1: 47, Player 2: 48\n",
      "Player 2 wins!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate MCTS against Random\n",
    "println(\"Evaluating MCTS against Random:\")\n",
    "evaluate_policies(mdp, mcts_policy, random_policy, 200)\n",
    "\n",
    "println(\"\\nPlaying an example game (MCTS vs Random):\")\n",
    "play_example_game(mdp, mcts_policy, random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already observe a very strong performance against a model making random moves, due to the big influence of the dice rolls even a perfect player has always a chance of loosing. Now let's see if we can improve the performance using Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "[Q-Learning](https://en.wikipedia.org/wiki/Q-learning) is one of the standard algorithms in Reinforcement Learning and estimates the expected value of each action state pair. The necessary functions for the training are quickly implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_qlearning (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define Q-learning policy\n",
    "mutable struct QLearningPolicy <: Policy\n",
    "    Q::Dict{Tuple{KnucklebonesState, Int}, Float64}\n",
    "    ϵ::Float64  # Epsilon for ϵ-greedy exploration\n",
    "end\n",
    "\n",
    "function QLearningPolicy(ϵ::Float64 = 0.1)\n",
    "    return QLearningPolicy(Dict{Tuple{KnucklebonesState, Int}, Float64}(), ϵ)\n",
    "end\n",
    "\n",
    "function POMDPs.action(policy::QLearningPolicy, s::KnucklebonesState)\n",
    "    if rand() < policy.ϵ\n",
    "        return rand(1:6)\n",
    "    else\n",
    "        return argmax(a -> get(policy.Q, (s, a), 0.0), 1:6)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Q-learning update function\n",
    "function update!(policy::QLearningPolicy, s::KnucklebonesState, a::Int, r, s_next::KnucklebonesState, α::Float64, γ::Float64)\n",
    "    current_q = get(policy.Q, (s, a), 0.0)\n",
    "    next_max_q = maximum(get(policy.Q, (s_next, a_next), 0.0) for a_next in 1:6)\n",
    "    policy.Q[(s, a)] = current_q + α * (r + γ * next_max_q - current_q)\n",
    "end\n",
    "\n",
    "# Training function for Q-learning\n",
    "function train_qlearning(mdp::KnucklebonesMDP, n_episodes::Int, α::Float64, γ::Float64, ϵ::Float64)\n",
    "    policy = QLearningPolicy(ϵ)\n",
    "    \n",
    "    for episode in 1:n_episodes\n",
    "        s = initialstate(mdp)\n",
    "        while !isterminal(mdp, s)\n",
    "            a = action(policy, s)\n",
    "            s_next, r = gen(mdp, s, a, Random.GLOBAL_RNG)\n",
    "            update!(policy, s, a, r, s_next, α, γ)\n",
    "            s = s_next\n",
    "        end\n",
    "        \n",
    "        if episode % 10000 == 0\n",
    "            println(\"Completed episode $episode\")\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Q-learning policy...\n",
      "Completed episode 10000\n",
      "Completed episode 20000\n",
      "Completed episode 30000\n",
      "Completed episode 40000\n",
      "Completed episode 50000\n",
      "Completed episode 60000\n",
      "Completed episode 70000\n",
      "Completed episode 80000\n",
      "Completed episode 90000\n",
      "Completed episode 100000\n",
      "\n",
      "Evaluating Q-learning against Random:\n",
      "Policy 1 wins: 109\n",
      "Policy 2 wins: 86\n",
      "Draws: 5\n"
     ]
    }
   ],
   "source": [
    "# Train Q-learning policy\n",
    "println(\"Training Q-learning policy...\")\n",
    "qlearning_policy = train_qlearning(mdp, 100000, 0.1, 0.95, 0.1)\n",
    "\n",
    "# Evaluate Q-learning against Random\n",
    "println(\"\\nEvaluating Q-learning against Random:\")\n",
    "evaluate_policies(mdp, qlearning_policy, random_policy, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It quickly shows that the Q-Learning approach leads to a weaker model than MCTS. The probable cause is the vastness of the possible game states, making identical action state pairs across repeated games very unlikely and Q-Learning ineffective at modelling the true expected value for the reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Network\n",
    "\n",
    "To counteract the problem of a vast state space, DeepMind developed the (DQN algorithm)[https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf], which combines traditional Reinforcement Learning with Deep Neural Networks. We will use the Machine Learning library [Flux.jl](https://fluxml.ai/Flux.jl/stable/) to implement the Deep Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_dqn (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mutable struct DQNPolicy\n",
    "    q_network::Chain\n",
    "    ϵ::Float64  # Exploration rate\n",
    "end\n",
    "\n",
    "# Epsilon-greedy action selection\n",
    "function POMDPs.action(policy::DQNPolicy, s::KnucklebonesState)\n",
    "    if rand() < policy.ϵ\n",
    "        return rand(1:6)\n",
    "    else\n",
    "        q_values = policy.q_network(state_to_vector(s))\n",
    "        return argmax(q_values)\n",
    "    end\n",
    "end\n",
    "\n",
    "# Update epsilon to reduce exploration over time\n",
    "function update_epsilon!(policy::DQNPolicy, min_ϵ::Float64, decay::Float64)\n",
    "    policy.ϵ = max(min_ϵ, policy.ϵ * decay)\n",
    "end\n",
    "\n",
    "# State to vector function\n",
    "function state_to_vector(s::KnucklebonesState)\n",
    "    # Flatten the board (2 players * 3 columns * 3 rows = 18 elements)\n",
    "    board_floats = Float32[s.board[player, col][row] for player in 1:2, col in 1:3, row in 1:3]\n",
    "    \n",
    "    # Add current player and current dice (2 more elements)\n",
    "    return vcat(vec(board_floats), Float32(s.current_player), Float32(s.current_dice))\n",
    "end\n",
    "\n",
    "# DQN training function\n",
    "function train_dqn(mdp, n_episodes::Int, batch_size::Int, ϵ::Float64)\n",
    "    state_dim = 20  # Assuming state is a 20-dimensional vector\n",
    "    action_dim = 6   # Assuming 6 possible actions\n",
    "    \n",
    "    # Define the Q-network (input: state_dim, output: action_dim)\n",
    "    q_network = Chain(\n",
    "        Dense(state_dim, 64, relu),\n",
    "        Dense(64, 64, relu),\n",
    "        Dense(64, action_dim)\n",
    "    )\n",
    "    \n",
    "    target_network = deepcopy(q_network)\n",
    "    optimizer = ADAM(0.001)\n",
    "    \n",
    "    replay_buffer = []\n",
    "    policy = DQNPolicy(q_network, ϵ)\n",
    "\n",
    "    # Training loop\n",
    "    for episode in 1:n_episodes\n",
    "        s = initialstate(mdp)  # Get the initial state of the episode\n",
    "        \n",
    "        while !isterminal(mdp, s)\n",
    "            # Select action based on epsilon-greedy policy\n",
    "            state_vec = state_to_vector(s)\n",
    "            a = select_action(policy, state_vec)\n",
    "            \n",
    "            # Take the action, observe the next state and reward\n",
    "            s_next, r = gen(mdp, s, a, Random.GLOBAL_RNG)\n",
    "            \n",
    "            # Add experience to replay buffer\n",
    "            push!(replay_buffer, (state_to_vector(s), a, r, state_to_vector(s_next), isterminal(mdp, s_next)))\n",
    "            if length(replay_buffer) > 10000\n",
    "                popfirst!(replay_buffer)  # Maintain buffer size\n",
    "            end\n",
    "            \n",
    "            s = s_next  # Move to the next state\n",
    "            \n",
    "            # Perform training if buffer has enough experiences\n",
    "            if length(replay_buffer) >= batch_size\n",
    "                batch_indices = rand(1:length(replay_buffer), batch_size)\n",
    "                batch = replay_buffer[batch_indices]\n",
    "\n",
    "                # Prepare batch data (states, actions, rewards, next states, and terminal flags)\n",
    "                states = hcat([b[1] for b in batch]...)\n",
    "                actions = [b[2] for b in batch]\n",
    "                rewards = [b[3] for b in batch]\n",
    "                next_states = hcat([b[4] for b in batch]...)\n",
    "                dones = [b[5] for b in batch]\n",
    "\n",
    "                # Calculate current Q-values\n",
    "                current_q_values = q_network(states)\n",
    "\n",
    "                # Calculate target Q-values using the target network\n",
    "                next_q_values = target_network(next_states)\n",
    "                max_next_q_values = [maximum(next_q) for next_q in eachcol(next_q_values)]\n",
    "                targets = [r + (1 - d) * 0.99 * max_q for (r, d, max_q) in zip(rewards, dones, max_next_q_values)]\n",
    "                \n",
    "                # Update Q-values for the taken actions\n",
    "                q_updates = copy(current_q_values)\n",
    "                for i in 1:batch_size\n",
    "                    q_updates[actions[i], i] = targets[i]\n",
    "                end\n",
    "\n",
    "                # Define the loss function\n",
    "                function compute_loss()\n",
    "                    return Flux.mse(current_q_values, q_updates)\n",
    "                end\n",
    "\n",
    "                # Manually calculate gradients and update parameters\n",
    "                grads = Flux.gradient(() -> compute_loss(), Flux.params(q_network))\n",
    "\n",
    "                # Perform the optimizer step\n",
    "                Flux.Optimise.update!(optimizer, Flux.params(q_network), grads)\n",
    "\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # Update the target network every 100 episodes\n",
    "        if episode % 100 == 0\n",
    "            target_network = deepcopy(q_network)\n",
    "        end\n",
    "        \n",
    "        # Decay epsilon to reduce exploration over time\n",
    "        update_epsilon!(policy, 0.1, 0.995)\n",
    "        \n",
    "        # Print progress every 1000 episodes\n",
    "        if episode % 10000 == 0\n",
    "            println(\"Completed episode $episode, ϵ: $(policy.ϵ)\")\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return policy\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DQN policy...\n",
      "Completed episode 10000, ϵ: 0.1\n",
      "Completed episode 20000, ϵ: 0.1\n",
      "Completed episode 30000, ϵ: 0.1\n",
      "Completed episode 40000, ϵ: 0.1\n",
      "Completed episode 50000, ϵ: 0.1\n",
      "Completed episode 60000, ϵ: 0.1\n",
      "Completed episode 70000, ϵ: 0.1\n",
      "Completed episode 80000, ϵ: 0.1\n",
      "Completed episode 90000, ϵ: 0.1\n",
      "Completed episode 100000, ϵ: 0.1\n",
      "\n",
      "Evaluating DQN against Random:\n",
      "Policy 1 wins: 95\n",
      "Policy 2 wins: 98\n",
      "Draws: 7\n"
     ]
    }
   ],
   "source": [
    "# Train DQN policy\n",
    "println(\"Training DQN policy...\")\n",
    "dqn_policy = train_dqn(mdp, 100000, 32, 0.1)\n",
    "\n",
    "# Evaluate DQN against Random\n",
    "println(\"\\nEvaluating DQN against Random:\")\n",
    "evaluate_policies(mdp, dqn_policy, random_policy, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deep neural network did not seem to improve the model performance of Q-Learning at all, so both models should be no match for the original MCTS model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating policies (MCTS vs Q-learning)\n",
      "Policy 1 wins: 150\n",
      "Policy 2 wins: 47\n",
      "Draws: 3\n",
      "\n",
      "Evaluating policies (Q-learning vs MCTS)\n",
      "Policy 1 wins: 48\n",
      "Policy 2 wins: 149\n",
      "Draws: 3\n",
      "\n",
      "Evaluating policies (MCTS vs DQN)\n",
      "Policy 1 wins: 157\n",
      "Policy 2 wins: 41\n",
      "Draws: 2\n",
      "\n",
      "Evaluating policies (DQN vs MCTS)\n",
      "Policy 1 wins: 46\n",
      "Policy 2 wins: 153\n",
      "Draws: 1\n",
      "\n",
      "Evaluating policies (Q-learning vs DQN)\n",
      "Policy 1 wins: 93\n",
      "Policy 2 wins: 99\n",
      "Draws: 8\n",
      "\n",
      "Evaluating policies (DQN vs Q-learning)\n",
      "Policy 1 wins: 115\n",
      "Policy 2 wins: 83\n",
      "Draws: 2\n"
     ]
    }
   ],
   "source": [
    "println(\"\\nEvaluating policies (MCTS vs Q-learning)\")\n",
    "evaluate_policies(mdp, mcts_policy, qlearning_policy, 200)\n",
    "\n",
    "println(\"\\nEvaluating policies (Q-learning vs MCTS)\")\n",
    "evaluate_policies(mdp, qlearning_policy, mcts_policy, 200)\n",
    "\n",
    "println(\"\\nEvaluating policies (MCTS vs DQN)\")\n",
    "evaluate_policies(mdp, mcts_policy, dqn_policy, 200)\n",
    "\n",
    "println(\"\\nEvaluating policies (DQN vs MCTS)\")\n",
    "evaluate_policies(mdp, dqn_policy, mcts_policy, 200)\n",
    "\n",
    "println(\"\\nEvaluating policies (Q-learning vs DQN)\")\n",
    "evaluate_policies(mdp, qlearning_policy, dqn_policy, 200)\n",
    "\n",
    "println(\"\\nEvaluating policies (DQN vs Q-learning)\")\n",
    "evaluate_policies(mdp, dqn_policy, qlearning_policy, 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that DQN outperforms Q-learning, but both are no match for MCTS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Mia Müßig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
